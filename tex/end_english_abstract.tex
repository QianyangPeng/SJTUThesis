%# -*- coding: utf-8-unix -*-
\begin{bigabstract}
This paper discussed the problem of the implementation and optimization of large-scaled academic searching platform. To be more concise, is to implement the 2.0 version of the searching platform for the academic website Acemap. On the first prototype of the searching platform, which is implemented before this study, its indexed was incomplete and the relevant functions were insufficient. The reason for these deficiencies is the database structure had been complicated, which created a huge I/O bound for the data import process. Moreover, the over-use of example code had made it difficult to develop self defined functions.

The initial motivation of this study is to settle these problems and implement a new version of well-functioned academic searching platform. On the one hand, my paper needs to find a algorithm to accelerate the data import speed by at least 100 times. On the other hand, I want to develop a complete searching platform from zero to fit in our functional demands, and to implement it on distributed architecture to enlarge its robustness and maintainability.

On the theoretical part, this study aims to develop a knowledge graph module based on the searching platform. The target is, when users input the keyword to submit their queries, our platform can automatically generate a knowledge graph indicating the relevant academic keyword hierarchy, to better assist users for their research purpose.

Alike the 1.0 version of our search platform, we use open source search platform \emph{Solr} to develop our system. And in the distributed platform, we use \emph{Zookeeper}, a distributed service coordination platform, to manage and deposit our server nodes. Based on these tools, we implemented the 2.0 version of our search platform. This work includes the design of system architecture and index structure; fast data import algorithms. platform background and foreground development and the platform's distributed deployment. Now I will describe these works one by one.

The first work is the design of system architecture. Our system architecture contains eight parts, including the website view, website controller, query preprocess, query tokenizer, database, document preprocesser, document tokenizer and search engine server. Compared to the old version, our new version of system is having more indexed fields, which means the index structure is more complicated but supporting more query methods and richer result fields. Also, the new version implemented a complete preprocess and tokenize to the documents and queries, which handled the problem of stopwords, singular-plural pair and language tense. To speed up the query process, I used the memory project file technology to implement the file system of solr's index file folder. It is to put all the index file into memory rather than hard disk, thus making the speed of reading the index file much more faster. As a result, this new method achieved a over 3 times boost in the query speed.

The second work is fast data import algorithm. We innovatively used a file import method in the data import process thus prevented the time-consuming I/O operations. In this part we firstly exported the whole database tables to files, then we used a two level dictionary to load these files into memory. For all the data import process involving table JOIN operation, we replaced them with similar dictionary lookup operation, and we merge our all dictionaries together to form a expanded dictionary. Finally, we write our expanded dictionary into a formatted single .xml file including all the needed fields, and used this .xml file to do our data import. By changing the hard drive reading to memory reading, the data import speed is increased by over 100 times, and the difficult problem of slow data import is solved.

The third work is the platform background and foreground development. The foreground of platform is the UI interface presented as a website, and the background is the controller of the website, together with the searching platform itself. When the users' input is valid, their input on the foreground user interface is sent to the background controller, and in the background controller the query is preprocessed and sent to the search engine. Then, a HTTP response in .json format is returned back to the website controller, processed and showed in the foreground UI. Also, this part of work includes the principle of keyword highlighting and result faceting, and how these functions are implemented into the system.

The last work is the platform's distributed deployment. A singled system just works fine, but is also having its deficiencies. When the computer resource, including memory, CPU and hard disk, is highly occupied, the servers response speed will be greatly delayed. Moreover, once the server is shut down, all the service will be suddenly down and unable to work. A distributed system can solve all these problems, and a well-developed distributed system can achieve config file centralized management, workload arrangement and can handle server nodes down and recover. We used Zookeeper to implement our distributed system. In this part, we described the architecture, deployment, configure and usage of our distributed system. Our final distributed system contains 1 collection, 2 shards and 2 replicas for each shard. Each of the two server include one copy of replica for both of two shards. So, when any server shuts down, we also have one server with a complete copy of index file that can handle all the searching work. For the most time when all the 4 server nodes are alive, Zookeeper where automatically elect a leader node and the leader node will undertake the shard management and workload distribution. When any server node is dead but started again, it will also automatically login back to the cloud without any extra configuration.

After these work, the 2.0 version of searching platform is completed and can be slated to launch on the website. Then I focused on the development of the knowledge graph module. Knowledge graph is a knowledge management tool that makes organizational processes more visible, feasible, and practicable.\citen{ddm} My motivation of developing this module is stated below: When users are using an academic website, they may only focus on the top few columns of his search result. However, most information buried under thousands of more results are just ignored. So I want to develop a function to summarize and to convert these results into a visible form, and we selected the form to be a layered knowledge hierarchy graph. Detailed examples are listed in the paper. To settle this problem, we used a tree generate algorithm and tree formalization algorithm to generate the basic knowledge graph, and used a set of scale control algorithms to make sure our graph is having proper size. As we want a dynamic knowledge graph generating process, which requires a relatively low algorithm complixity and quick response, so some of the algorithms with low converge speed are not applied in the system. On the visualization of our knowledge graph, we used the sunburst effect provided by d3.js to draw it on the website. Some other elements are also added next to the graph to provide more information.

In my graduate design I successfully implemented the 2.0 version of our search system, which achieved a over 3 times query speed and a over 100 times data-import speed compared to the 1.0 version. Moreover, I implemented the whole system in distributed mode thus achieving a higher robustness and efficiency. It is glad to see all problems I want to solve having been solved well, and in the process of solving these problems, I had a more well-rounded recognition to the application of search engine. So Thanks for the chance my school providing me to have this graduate design, which have made me learned quite a lot and motivated me to be brave to face to any challenge in front of me.

\end{bigabstract}